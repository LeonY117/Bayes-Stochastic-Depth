{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48081,"status":"ok","timestamp":1696880877219,"user":{"displayName":"Leon Yao","userId":"15846177251700038630"},"user_tz":-60},"id":"N1zftQgOsfr3","outputId":"32826dd4-eca6-445f-b138-f1ebd5d69ee8"},"outputs":[],"source":["import os\n","import sys\n","\n","try:\n","  ENV = 'local' if os.environ.get('COLAB') else 'colab'\n","except:\n","  ENV = 'colab'\n","\n","if ENV == 'colab':\n","  # for running on google drive:\n","  from google.colab import drive\n","  drive.mount(\"/content/drive/\", force_remount=True)\n","\n","  !pip install fvcore -q\n","  !pip install -e \"/content/drive/My Drive/Bayes-Stochastic-Depth\" -q\n","\n","  module_dir = \"/content/drive/My Drive/Bayes-Stochastic-Depth/src\"\n","  # append local module to path\n","  module_path = os.path.abspath(os.path.join(module_dir))\n","  if module_path not in sys.path:\n","      sys.path.append(module_path)\n","\n","  data_dir = \"/content/drive/My Drive/Bayes-Stochastic-Depth/data\"\n","\n","elif ENV == 'local':\n","  data_dir = \"../data\""]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11917,"status":"ok","timestamp":1696880889129,"user":{"displayName":"Leon Yao","userId":"15846177251700038630"},"user_tz":-60},"id":"Q0efNAAKrPBD"},"outputs":[],"source":["from typing import Any, Callable, List, Optional, Type, Union, Tuple, Dict\n","from torch import Tensor\n","\n","import torch\n","from torch import nn, optim, mps\n","from torch.utils.data import DataLoader, Dataset\n","import torchvision\n","from torchvision import transforms\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import copy\n","from tqdm import tqdm\n","\n","from models import resnet\n","from data import get_dataset\n","from visualization import show_cifar_images\n","from utils import (\n","    AccuracyMetric,\n","    get_confusion_matrix,\n","    count_FLOPS,\n","    count_parameters,\n","    calculate_storage_in_mb,\n","    bayes_eval,\n","    bayes_forward,\n","    parse_loss,\n","    parse_scheduler,\n","    parse_optimizer,\n","    get_dataset_classes\n",")"]},{"cell_type":"markdown","metadata":{"id":"Tde4ZS79rPBE"},"source":["## Prepare dataset\n","\n","### Note on moving dataset into memory:\n","\n","Since the entire CIFAR dataset (train + val + test) is around 180 MB, this can easily fit onto the GPU directly & speed up training a lot.\n","\n","**Comparison**\n","* Training time (per epoch) with data stored on CPU: `60s`\n","* Everything on MPS: `40s`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40947,"status":"ok","timestamp":1696880930072,"user":{"displayName":"Leon Yao","userId":"15846177251700038630"},"user_tz":-60},"id":"4ODaFC1prPBF","outputId":"a8f3a496-5c6f-42e3-fe22-ec9074fa1156"},"outputs":[],"source":["# some global variables\n","classes = get_dataset_classes(\"cifar10\")\n","\n","FILL_PIX = None\n","\n","DEVICE = torch.device(\"cpu\")\n","if torch.backends.mps.is_available():\n","    DEVICE = torch.device(\"mps\")\n","if torch.cuda.is_available():\n","    DEVICE = torch.device(\"cuda:0\")\n","\n","# overwrite device:\n","# DEVICE = torch.device(\"cpu\")\n","print(f'Using Device {DEVICE}')\n","\n","datasets = get_dataset('cifar10', data_dir, DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"j6r7omYFrPBG"},"source":["### Preview images"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":1366,"status":"ok","timestamp":1696880931414,"user":{"displayName":"Leon Yao","userId":"15846177251700038630"},"user_tz":-60},"id":"P5_AT62lrPBH","outputId":"8b2957bb-295d-45e1-8e64-52518c5be4fb"},"outputs":[],"source":["show_cifar_images(dataset=datasets['train'], grid_size=(2, 8), show_labels=True)\n","\n","# to show images with predictions:\n","# show_cifar_images((2, 8), show_labels=True, dataset=testset, preds=preds)"]},{"cell_type":"markdown","metadata":{"id":"bkqWnthNrPBH"},"source":["## ResNet preview"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"urM-OB24rPBH"},"outputs":[],"source":["resnet18 = resnet(\"resnet18\", num_classes=10)\n","\n","# load resnet18 pretrained on ImageNet\n","# resnet18_torch = torchvision.models.resnet.resnet18(num_classes=10)\n","\n","print(f\"Number of parameters: {count_parameters(resnet18, print_table=False)/1e6 :.3f} M\")\n","print(f\"Parameter in MB: {calculate_storage_in_mb(resnet18):.3f}\")\n","print(f\"Number of expected layers: {resnet18.expected_layers()}\")\n","# print(f\"Number of FLOPS: {count_FLOPS(resnet18, input_dim=(32, 32))/1e9:.3f} GFLOPS\")\n","\n","# dummy_input = torch.randn(16, 3, 32, 32)\n","# dummy_output = resnet18(dummy_input)\n","\n","del(resnet18)"]},{"cell_type":"markdown","metadata":{"id":"IsR4VIX8rPBI"},"source":["## Training Manager"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1696880931414,"user":{"displayName":"Leon Yao","userId":"15846177251700038630"},"user_tz":-60},"id":"0WbK7Q-SrPBI"},"outputs":[],"source":["class TrainingManager:\n","    def __init__(self, config, datasets):\n","        self.datasets = datasets\n","        self.config = config\n","\n","        self._load()\n","\n","    def _load(self) -> None:\n","        # read config\n","        self.dataset = self.config.get(\"dataset\", \"CIFAR10\")\n","        self.classes = get_dataset_classes(self.dataset)\n","        self.num_classes = len(self.classes)\n","        self.T = self.config.get(\"T\", 10)\n","        self.batch_size = self.config.get(\"batch_size\", 16)\n","        self.input_dim = self.config.get(\"input_dim\", (32, 32))\n","        self.total_epochs = self.config.get(\"total_epochs\", 100)\n","        self.weight_decay = self.config.get(\"weight_decay\", 1e-4)\n","        self.baseline_lr = self.config.get(\"lr\", 1e-4)\n","        self.patience = self.config.get(\"patience\", 20)\n","        self.track_metrics = self.config.get(\"track_metrics\", [\"loss\"])\n","        self.num_classes = self.config.get(\"num_classes\", 10)\n","\n","        # initiate dataloader\n","        self.dataloaders = {\n","            \"train\": DataLoader(\n","                self.datasets[\"train\"], batch_size=self.batch_size, shuffle=True\n","            ),\n","            \"val\": DataLoader(\n","                self.datasets[\"val\"], batch_size=self.batch_size, shuffle=False\n","            ),\n","            \"test\": DataLoader(\n","                self.datasets[\"test\"], batch_size=self.batch_size, shuffle=False\n","            ),\n","        }\n","\n","        # initiate model\n","        self.net = resnet(\n","            resnet_name=self.config[\"model\"],\n","            num_classes=self.config.get(\"num_classes\", 10),\n","            dropout_mode=self.config.get(\"dropout_mode\", \"none\"),\n","            dropout_p=self.config.get(\"dropout_p\", 0.0),\n","            sd_mode=self.config.get(\"sd_mode\", \"none\"),\n","            sd_p=self.config.get(\"sd_p\", 0.0),\n","        )\n","        self.net.to(DEVICE)\n","\n","        # initiate loss function and metric\n","        self.criterion = parse_loss(self.config.get(\"loss\", \"CE\"))\n","        self.metric = AccuracyMetric(metrics=[\"acc\"], classes=self.classes)\n","\n","        # initiate optimizer and scheduler\n","        self.optimizer = parse_optimizer(\n","            self.net,\n","            self.config.get(\"optimizer\", \"sgd\"),\n","            self.baseline_lr,\n","            self.weight_decay,\n","            momentum = self.config.get(\"momentum\", 0.9)\n","        )\n","\n","        self.scheduler = parse_scheduler(\n","            self.optimizer,\n","            self.config.get(\"scheduler\", \"none\"),\n","            self.total_epochs,\n","            milestones=self.config.get(\"milestones\", None),\n","            gamma=self.config.get(\"gamma\", None),\n","        )\n","\n","        # initialize history\n","        self.epoch = 0\n","        self.history = {\"lr\": []}\n","\n","        for split in [\"train\", \"val\", \"val_b\"]:\n","            self.history[f\"loss/{split}\"] = []\n","            self.history[f\"acc/avg/{split}\"] = []\n","            self.history[f\"acc/global/{split}\"] = []\n","            for c in self.classes:\n","                self.history[f\"acc/{c}/{split}\"] = []\n","\n","        self.history[\"best_val_loss\"] = 999\n","        self.history[\"best_val_acc\"] = 0\n","        self.history[\"best_epoch\"] = 0\n","        self.patience_count = 0\n","\n","    def _check_early_stop(self, min_epochs: int = 10) -> bool:\n","        \"\"\"\n","        Checks for early stop and updates best net, returns early stop state\n","        \"\"\"\n","        if self.epoch < min_epochs or len(self.history[\"loss/val\"]) <= 1:\n","            return False\n","\n","        update_best = False\n","        loss = self.history[\"loss/val\"][-1]\n","        acc = self.history[\"acc/avg/val\"][-1]\n","        curr_min_loss = self.history.get(\"best_val_loss\", 999)\n","        curr_max_acc = self.history.get(\"best_val_acc\", 0)\n","\n","        # compare loss and acc to current best\n","        if \"loss\" in self.track_metrics and loss < curr_min_loss:\n","            print(f\"loss decreased by {(curr_min_loss-loss)/curr_min_loss*100 :.3f} %\")\n","            self.history[\"best_val_loss\"] = loss\n","            update_best = True\n","        if \"acc\" in self.track_metrics and acc > curr_max_acc:\n","            print(\n","                f\"acc increased by {(acc-curr_max_acc)/(curr_max_acc+1e-16)*100 :.3f} %\"\n","            )\n","            self.history[\"best_val_acc\"] = acc\n","            update_best = True\n","\n","        if loss > curr_min_loss and acc < curr_max_acc:\n","            self.patience_count += 1\n","\n","        # update best net\n","        if update_best:\n","            print(\"saving best net...\")\n","            self.best_net = copy.deepcopy(self.net)\n","            self.history[\"best_epoch\"] = self.epoch\n","            self.patience_count = 0\n","\n","        if self.patience_count >= self.patience:\n","            print(\n","                f\"Acc(c) or miou have not improved for {self.patience} epochs, terminate training\"\n","            )\n","            return True\n","\n","        return False\n","\n","    def evaluate(\n","        self,\n","        set: str = \"test\",\n","        bayes_mode: bool = False,\n","        T: Optional[int] = None,\n","        **kwargs,\n","    ) -> Tuple[float, Dict[str, float]]:\n","        \"\"\"\n","        Evaluate the model on a given set\n","        \"\"\"\n","        self.net.eval()  # must be called before setting bayes mode\n","        if bayes_mode:\n","            self.net.set_bayes_mode(True)\n","            T = self.T if T is None else T\n","            assert T > 0, \"T must be greater than 0 if bayes_mode is True\"\n","\n","            h, w = self.config[\"input_dim\"]\n","            buffer_tensor = torch.empty(\n","                size=(T, 3, h, w), dtype=torch.float32, device=DEVICE\n","            )\n","        else:\n","            self.net.set_bayes_mode(False)\n","            T = 0\n","            buffer_tensor = None\n","\n","        loss = 0\n","        confusion_matrix = torch.zeros(\n","            (self.num_classes, self.num_classes), device=DEVICE\n","        )\n","\n","        with torch.no_grad():\n","            for X_batch, y_batch in tqdm(self.dataloaders[set]):\n","                if bayes_mode:\n","                    # force batch size to be 1\n","                    for x, y in zip(X_batch, y_batch):\n","                        # forward pass\n","                        y_logits, y_pred = bayes_eval(\n","                            self.net, x, T, buffer=buffer_tensor\n","                        )\n","                        # compute loss\n","                        loss += self.criterion(y, y_logits).item()\n","                        # update confusion matrix\n","                        confusion_matrix += get_confusion_matrix(\n","                            y, y_pred, self.num_classes\n","                        )\n","                elif not bayes_mode:\n","                    # regular forward pass over the batch\n","                    batch_size = len(X_batch)\n","                    # forward pass\n","                    y_logits = self.net(X_batch)\n","                    # compute loss\n","                    loss += self.criterion(y_batch, y_logits).item() * batch_size\n","                    # update confusion matrix\n","                    y_pred = torch.argmax(y_logits, 1, keepdim=False)\n","                    confusion_matrix += get_confusion_matrix(\n","                        y_batch, y_pred, self.num_classes\n","                    )\n","\n","        # compute loss and accuracy\n","        loss /= len(self.dataloaders[set].dataset)\n","        accs = self.metric(confusion_matrix)\n","\n","        return loss, accs\n","\n","    def train(self, epochs, eval_mode=\"bayes\") -> None:\n","        \"\"\"\n","        Main training loop\n","        Args:\n","            epochs: number of epochs to train\n","            eval_mode: mode to evaluate the model, can be \"regular\", \"bayes\", or \"all\"\n","        \"\"\"\n","        torch.cuda.empty_cache()  # helps clearing RAM\n","        for e in range(epochs):\n","            self.net.train()\n","            # stochastic regularization should always be on during training\n","            self.net.set_bayes_mode(True)\n","            # create temporary dict to hold epoch results for train set\n","            train_loss = 0\n","            confusion_matrix = torch.zeros(\n","                (self.num_classes, self.num_classes), device=DEVICE\n","            )\n","\n","            for X_batch, y_batch in tqdm(self.dataloaders[\"train\"]):\n","                self.optimizer.zero_grad()\n","                # forward pass\n","                y_logits = self.net(X_batch)\n","                # compute loss\n","                loss = self.criterion(y_batch, y_logits)\n","                # back prop\n","                loss.backward()\n","                # update parameters\n","                self.optimizer.step()\n","                # update loss\n","                batch_size = X_batch.shape[0]\n","                train_loss += loss.item() * batch_size\n","                # update confusion matrix\n","                y_pred = torch.argmax(y_logits, 1, keepdim=False)\n","                confusion_matrix += get_confusion_matrix(\n","                    y_batch, y_pred, self.num_classes\n","                )\n","            self.scheduler.step()\n","\n","            # compute average train loss and accuracy\n","            n_train = len(self.dataloaders[\"train\"].dataset)\n","            train_loss /= n_train\n","            accs = self.metric(confusion_matrix)\n","            self._log_history(\"train\", train_loss, accs)\n","\n","            # compute validation loss and accuracy\n","            val_loss, val_accs, val_loss_b, val_accs_b = None, None, None, None\n","            if eval_mode == \"regular\" or eval_mode == \"all\":\n","                val_loss, val_accs = self.evaluate(\"val\", bayes_mode=False)\n","            if eval_mode == \"bayes\" or eval_mode == \"all\":\n","                val_loss_b, val_accs_b = self.evaluate(\"val\", bayes_mode=True)\n","\n","            self._log_history(\"val\", val_loss, val_accs)\n","            self._log_history(\"val_b\", val_loss_b, val_accs_b)\n","\n","            self.history[\"lr\"].append(self.scheduler.get_last_lr()[0])\n","\n","            # check for early stop\n","            early_stop = self._check_early_stop()\n","            self.epoch += 1\n","\n","            # print results\n","            print(f\"Epoch {self.epoch}\")\n","            for name in [\"train\", \"val\"]:\n","                print(\n","                    f\"{name}: Acc(g) = {self.history[f'acc/global/{name}'][-1]*100:.4f}, Acc(c) = {self.history[f'acc/avg/{name}'][-1]*100:.4f}, Loss = {self.history[f'loss/{name}'][-1]:.4f}\"\n","                )\n","\n","            if early_stop or self.epoch >= self.total_epochs:\n","                print(\"Terminate training\")\n","                return\n","\n","    def _log_history(self, set: str, loss: float, accs: Dict[str, float]) -> None:\n","        loss = loss if loss is not None else 0\n","        accs = accs if accs is not None else {}\n","        self.history[f\"loss/{set}\"].append(loss)\n","        for name in accs.keys():\n","            self.history[f\"{name}/{set}\"].append(accs.get(name, 0))\n","        return\n","\n","    def load(self, dir: str) -> None:\n","        \"\"\"\n","        Load a model from a given directory\n","        \"\"\"\n","        net_path = os.path.join(dir, \"model.pt\")\n","        best_net_path = os.path.join(dir, \"best_model.pt\")\n","        history_path = os.path.join(dir, \"history.json\")\n","        config_path = os.path.join(dir, \"config.json\")\n","\n","        if not os.path.exists(history_path):\n","            raise ValueError(f\"Could not find history at {history_path}\")\n","        if not os.path.exists(config_path):\n","            raise ValueError(f\"Could not find config at {config_path}\")\n","\n","        with open(history_path, \"r\") as f:\n","            self.history = json.load(f)\n","        with open(config_path, \"r\") as f:\n","            self.config = json.load(f)\n","\n","        self._load()\n","\n","        if not os.path.exists(net_path):\n","            raise ValueError(f\"Could not find model at {net_path}\")\n","        self.net = torch.load(net_path).to(self.device)\n","\n","        if not os.path.exists(best_net_path):\n","            self.best_net = None\n","        else:\n","            self.best_net = torch.load(best_net_path).to(self.device)\n","\n","        print(f\"Loaded model from {dir}\")"]},{"cell_type":"markdown","metadata":{"id":"hTH1_0wfsoQL"},"source":["## Result Manager"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1696880931414,"user":{"displayName":"Leon Yao","userId":"15846177251700038630"},"user_tz":-60},"id":"GB-eXm4nmv9I"},"outputs":[],"source":["from visualization import (\n","    plot_loss_history,\n","    plot_acc_history,\n","    plot_per_class_acc_history,\n",")\n","from utils import get_dataset_classes, compute_calibration_errors, _get_calibration, _compute_calibration_statistics\n","import json\n","\n","\n","class ResultManager:\n","    def __init__(\n","        self,\n","        training_manager: TrainingManager,\n","        directory: Optional[str] = None,\n","    ) -> None:\n","        self.training_manager = training_manager\n","        self.datasets = training_manager.datasets\n","        self.config = training_manager.config\n","\n","        dataset_name = self.training_manager.config[\"dataset\"]\n","        self.classes = get_dataset_classes(dataset_name)\n","\n","        if directory is None:\n","            experiment = self.training_manager.config[\"experiment\"]\n","            self.directory = f\"results/{dataset_name}/{experiment}\"\n","        else:\n","            self.directory = directory\n","\n","    def save(self, **kwargs) -> None:\n","        \"\"\"\n","        Saves the training results to self.directory\n","        \"\"\"\n","        self._save_history(kwargs.pop(\"save_plot\", True))\n","        self._save_config()\n","        self._save_net(kwargs.pop(\"save_best_net\", False))\n","        self._save_results()\n","        self._save_calibration(\n","            num_buckets=kwargs.pop(\"num_buckets\", 10),\n","            ks=kwargs.pop(\"ks\", None),\n","            save_plot=kwargs.pop(\"save_plot\", False),\n","        )\n","\n","        return\n","\n","    def _save_history(self, save_plot=True) -> None:\n","        \"\"\"\n","        Saves the training history to self.directory/history.json\n","        \"\"\"\n","        history = self.training_manager.history\n","        with open(f\"{self.directory}/history.json\", \"w\") as f:\n","            json.dump(history, f, indent=2)\n","        print(f\"saved history to {self.directory}/history.json\")\n","        if save_plot:\n","            self.plot_history(history, save=True)\n","\n","    def _save_config(self) -> None:\n","        \"\"\"\n","        Saves the training config to self.directory/config.json\n","        \"\"\"\n","        config = self.training_manager.config\n","        with open(f\"{self.directory}/config.json\", \"w\") as f:\n","            json.dump(config, f, indent=2)\n","        print(f\"saved config to {self.directory}/config.json\")\n","\n","    def _save_net(self, save_best_net=False) -> None:\n","        \"\"\"\n","        Saves the trained model to self.directory/net.pt\n","        \"\"\"\n","        net = self.training_manager.net\n","        torch.save(net.state_dict(), f\"{self.directory}/model.pt\")\n","        if save_best_net:\n","            best_net = self.training_manager.best_net\n","            torch.save(best_net.state_dict(), f\"{self.directory}/best_model.pt\")\n","        print(f\"saved net to {self.directory}/net.pt\")\n","\n","    def _save_results(self) -> None:\n","        \"\"\"\n","        Runs evaluation on test set, then saves the results to self.directory/results.json\n","        \"\"\"\n","        # Retrieve information about training and validation set from history\n","        results = {}\n","        history = self.training_manager.history\n","        for split in [\"train\", \"val\"]:\n","            results[split] = {}\n","            for metric in [\"loss\", \"acc/avg\", \"acc/global\"]:\n","                results[split][metric] = history[f\"{metric}/{split}\"][-1]\n","            results[split][\"acc\"] = {}\n","            for c in self.classes:\n","                results[split][\"acc\"][f\"{c}\"] = history[f\"acc/{c}/{split}\"][-1]\n","\n","        # Test set\n","        print(\"Evaluating on test set...\")\n","        loss, accs = self.training_manager.evaluate(\"test\", bayes_mode=False)\n","        loss_b, accs_b = self.training_manager.evaluate(\"test\", bayes_mode=True)\n","\n","        results[\"test\"] = {\n","            \"loss\": loss,\n","            \"acc/global\": accs[\"acc/global\"],\n","            \"acc/avg\": accs[\"acc/avg\"],\n","            \"accs\": {},\n","        }\n","        results[\"test_bayes\"] = {\n","            \"loss\": loss_b,\n","            \"acc/global\": accs_b[\"acc/global\"],\n","            \"acc/avg\": accs_b[\"acc/avg\"],\n","            \"accs\": {},\n","        }\n","        for c in self.classes:\n","            results[\"test\"][\"accs\"][c] = accs[f\"acc/{c}\"]\n","            results[\"test_bayes\"][\"accs\"][c] = accs_b[f\"acc/{c}\"]\n","\n","        with open(f\"{self.directory}/results.json\", \"w\") as f:\n","            json.dump(results, f, indent=2)\n","        print(f\"saved results to {self.directory}/results.json\")\n","\n","    def _save_calibration(\n","        self,\n","        num_buckets: Optional[int] = 10,\n","        ks: Optional[List[int]] = None,\n","        save_plot: Optional[bool] = False,\n","    ) -> None:\n","        self.plot_calibration(num_buckets, ks, save_json=True, save_plot=save_plot)\n","\n","        # ECE, MCE = compute_calibration_errors(net, self.datasets[\"test\"], )\n","\n","    def plot_calibration(\n","        self,\n","        num_buckets: Optional[int] = 10,\n","        ks: Optional[List[int]] = None,  # include 0 for normal pass\n","        save_json: bool = False,\n","        save_plot: bool = False,\n","        dataloader: Optional[DataLoader] = None,\n","        net: Optional[nn.Module] = None,\n","    ) -> None:\n","        if ks is None:\n","            ks = [0, 5, 10]\n","\n","        if dataloader is None:\n","            dataloader = self.training_manager.dataloaders[\"test\"]\n","\n","        if net is None:\n","            net = self.training_manager.net\n","\n","        statistics = {}\n","\n","        plt.figure(figsize=(8, 6))\n","\n","        linear_line = np.array(range(num_buckets + 1)) / (num_buckets)\n","        plt.plot(linear_line, linear_line, color=\"black\", zorder=2)\n","\n","        print(f\"computing calibration for k={ks}\")\n","        for i, k in enumerate(ks):\n","            # get the buckets\n","            total_counts, total_corrects, confs = _get_calibration(\n","                net, dataloader, k=k, num_buckets=num_buckets\n","            )\n","            # compute statistics\n","            ECE, MCE = _compute_calibration_statistics(total_counts, total_corrects, confs)\n","            statistics[k] = {\"MCE\": float(MCE), \"ECE\": float(ECE)}\n","            # calculate the accuracy of each bucket\n","            acc = total_corrects / total_counts\n","\n","            plt.plot(confs, acc, label=f\"k={k}, ECE={ECE:.3f}\")\n","\n","        plt.xlim(0, 1)\n","        plt.ylim(0, 1)\n","        plt.xlabel(\"Confidence\")\n","        plt.ylabel(\"Accuracy\")\n","\n","        plt.legend()\n","\n","        if save_json:\n","            filepath = os.path.join(self.directory, \"calibration.json\")\n","            with open(filepath, \"w\") as f:\n","                json.dump(statistics, f, indent=2)\n","                print(f\"saved to {filepath}\")\n","\n","        if save_plot:\n","            filepath = os.path.join(self.directory, \"calibration.png\")\n","            plt.savefig(filepath, dpi=200)\n","            print(f\"saved to {filepath}\")\n","\n","        return statistics\n","\n","    def plot_history(\n","        self,\n","        history: Optional[Dict] = None,\n","        classes: List[str] = None,\n","        save: bool = False,\n","    ) -> None:\n","        \"\"\"\n","        Plots the training history\n","        ------------------------------\n","        Loss trainval | acc(g) trainval\n","        ------------------------------\n","        acc(c) train  | acc(c) val\n","        ------------------------------\n","        \"\"\"\n","        if history is None:\n","            history = self.training_manager.history\n","\n","        if classes is None:\n","            classes = self.classes\n","\n","        assert [\n","            x in history.keys()\n","            for x in [\"loss/train\", \"loss/val\", \"acc/global/train\", \"acc/global/val\"]\n","        ], \"history does not contain the required keys\"\n","\n","        plt.figure(figsize=(12, 10))\n","        plt.subplot(2, 2, 1)\n","        loss_history = {\"train\": history[\"loss/train\"], \"val\": history[\"loss/val\"]}\n","        plot_loss_history(loss_history)\n","        plt.title(\"Loss\")\n","\n","        plt.subplot(2, 2, 2)\n","        acc_history = {\n","            \"train\": history[\"acc/global/train\"],\n","            \"val\": history[\"acc/global/val\"],\n","        }\n","        plot_acc_history(acc_history)\n","        plt.title(\"Acc(g)\")\n","\n","        plt.subplot(2, 2, 3)\n","        per_class_acc_history = {cls: history[f\"acc/{cls}/train\"] for cls in classes}\n","        plot_per_class_acc_history(per_class_acc_history, classes)\n","        plt.title(\"Acc(c) Train\")\n","\n","        plt.subplot(2, 2, 4)\n","        per_class_acc_history = {cls: history[f\"acc/{cls}/val\"] for cls in classes}\n","        plot_per_class_acc_history(per_class_acc_history, classes)\n","        plt.title(\"Acc(c) Val\")\n","\n","        plt.tight_layout()\n","\n","        if save:\n","            plt.savefig(f\"{self.directory}/history.png\", dpi=200)\n","            print(f\"saved history to {self.directory}/history.png\")"]},{"cell_type":"markdown","metadata":{"id":"JgMOYwC3rPBJ"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":323,"status":"ok","timestamp":1696880938367,"user":{"displayName":"Leon Yao","userId":"15846177251700038630"},"user_tz":-60},"id":"J9hUUDscAPAZ"},"outputs":[],"source":["# TODO: refactor into utils\n","def _create_experiment_dir(\n","    config: Dict[str, Any], root_dir: str = \"/content/drive/MyDrive/Bayes-Stochastic-Depth\"\n",") -> str:\n","    dataset = config.get(\"dataset\", \"cifar10\")\n","    result_dir = f\"{root_dir}/results/{dataset}/\"\n","    exp = config[\"experiment\"]\n","\n","    experiment_dir = os.path.join(result_dir, exp)\n","\n","    if exp not in os.listdir(result_dir):\n","        os.mkdir(experiment_dir)\n","        print(f\"created directory {experiment_dir}\")\n","    else:\n","        num_items = len(os.listdir(experiment_dir))\n","        if num_items > 0:\n","            print(\"Warning: duplicate experiment name!\")\n","            print(\n","                f\"Directory at {experiment_dir} already exists and contains {num_items} items\"\n","            )\n","\n","    return experiment_dir"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":201,"status":"ok","timestamp":1696884811339,"user":{"displayName":"Leon Yao","userId":"15846177251700038630"},"user_tz":-60},"id":"xfixAhfZrPBJ","outputId":"73d06702-54e2-497f-a090-656e52deb5aa"},"outputs":[],"source":["config = {\n","    \"experiment\": \"resnet18_300e_classifier\",\n","    \"dataset\": \"cifar10\",  # from which we get classes separately\n","    \"input_dim\": (32, 32),\n","    \"model\": \"resnet18\",\n","    \"dropout_mode\": \"classifier\",\n","    \"dropout_p\": 0.5,\n","    \"sd_mode\": \"none\",\n","    \"sd_p\": 0.0,\n","    \"loss\": \"CE\",  # [\"CE\", \"Focal\"]\n","    \"optimizer\": \"sgd\",\n","    \"lr\": 0.1,\n","    \"weight_decay\": 1e-4,\n","    \"scheduler\": \"multistep\",  # [\"poly\", \"exp\", \"multistep\"],\n","    \"milestones\": [150, 225],\n","    \"gamma\": 0.1,\n","    \"T\": 10,\n","    \"batch_size\": 256,\n","    \"total_epochs\": 300,\n","    \"patience\": 300,\n","    \"track_metrics\": [\"loss\", \"acc\"],\n","}\n","\n","training_manager = TrainingManager(config, datasets)\n","result_manager = ResultManager(training_manager, _create_experiment_dir(config))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"dEt2RqZbqfvE"},"outputs":[],"source":["training_manager.train(300, eval_mode='regular')\n","result_manager.save(ks=[10])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lc_Cxy5BMBs-","outputId":"a69e96d4-6bee-478c-8c87-85700d465600"},"outputs":[],"source":["config = {\n","    \"experiment\": \"resnet50_300e_sd10\",\n","    \"dataset\": \"cifar10\",  # from which we get classes separately\n","    \"input_dim\": (32, 32),\n","    \"model\": \"resnet50\",\n","    \"dropout_mode\": \"none\",\n","    \"dropout_p\": 0.0,\n","    \"sd_mode\": \"constant\",\n","    \"sd_p\": 0.1,\n","    \"loss\": \"CE\",  # [\"CE\", \"Focal\"]\n","    \"optimizer\": \"sgd\",\n","    \"lr\": 0.1,\n","    \"weight_decay\": 1e-4,\n","    \"scheduler\": \"multistep\",  # [\"poly\", \"exp\", \"multistep\"],\n","    \"milestones\": [150, 225],\n","    \"gamma\": 0.1,\n","    \"T\": 10,\n","    \"batch_size\": 256,\n","    \"total_epochs\": 300,\n","    \"patience\": 300,\n","    \"track_metrics\": [\"loss\", \"acc\"],\n","}\n","\n","training_manager = TrainingManager(config, datasets)\n","result_manager = ResultManager(training_manager, _create_experiment_dir(config))\n","\n","training_manager.train(300, eval_mode='regular')\n","\n","result_manager.save(ks=[10])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":334503,"status":"ok","timestamp":1696784644776,"user":{"displayName":"Leon Yao","userId":"15846177251700038630"},"user_tz":-60},"id":"2LW2nUKXMPCU","outputId":"9bc67aff-a59e-4a58-bce9-8ea597f5ae3d"},"outputs":[],"source":["config = {\n","    \"experiment\": \"resnet50_10e_10sd\",\n","    \"description\": \"\",\n","    \"dataset\": \"cifar10\",  # from which we get classes separately\n","    \"input_dim\": (32, 32),\n","    \"model\": \"resnet50\",\n","    \"dropout_mode\": \"none\",\n","    \"dropout_p\": 0.0,\n","    \"sd_mode\": \"constant\",\n","    \"sd_p\": 0.1,\n","    \"loss\": \"CE\",  # [\"CE\", \"Focal\"]\n","    \"optimizer\": \"sgd\",\n","    \"lr\": 0.1,\n","    \"weight_decay\": 1e-4,\n","    \"scheduler\": \"multistep\",  # [\"poly\", \"exp\", \"multistep\"],\n","    \"milestones\": [150, 225],\n","    \"gamma\": 0.1,\n","    \"T\": 10,\n","    \"batch_size\": 256,\n","    \"total_epochs\": 300,\n","    \"patience\": 20,\n","    \"track_metrics\": [\"loss\", \"acc\"],\n","}\n","\n","training_manager = TrainingManager(config, datasets)\n","result_manager = ResultManager(training_manager, _create_experiment_dir(config))\n","\n","training_manager.train(10, eval_mode='regular')\n","\n","result_manager.save(ks=[10])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"elapsed":412,"status":"error","timestamp":1696857676326,"user":{"displayName":"Leon Yao","userId":"15846177251700038630"},"user_tz":-60},"id":"qnu7O-XhQFky","outputId":"0b78fd88-d5fe-4edf-83a5-32ca65dd27ff"},"outputs":[],"source":["result_manager = ResultManager(training_manager, _create_experiment_dir(config))\n","result_manager._save_results()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["bkqWnthNrPBH"],"name":"","toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
