{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Moon Classification with stochastic depth\n",
    "\n",
    "* We will take the two moon dataset as a dummy dataset that's easy to visualize, and just qualitatively see how the uncertainty boundaries differ compared to dropout.\n",
    "    * IF the results look good, we'll also try to implement other methods such as Laplace Approximation for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from torchvision.ops import StochasticDepth\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Moon dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, Y, train_ratio=0.8):\n",
    "    n = len(X)\n",
    "    n_train = int(n * train_ratio)\n",
    "    indices = torch.randperm(n)\n",
    "    train_indices, test_indices = indices[:n_train], indices[n_train:]\n",
    "    X_train, Y_train = X[train_indices], Y[train_indices]\n",
    "    X_test, Y_test = X[test_indices], Y[test_indices]\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "\n",
    "def plot_two_moons(X, Y):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    for i in range(len(X)):\n",
    "        plt.scatter(X[i, 0], X[i, 1], c=\"orange\" if Y[i] == 1 else \"green\")\n",
    "    # plt.axis(\"off\")\n",
    "\n",
    "\n",
    "X, Y = make_moons(n_samples=200, shuffle=True, noise=0.25, random_state=1)\n",
    "X, Y = torch.tensor(X, dtype=torch.float32), torch.tensor(Y, dtype=torch.float32)\n",
    "\n",
    "# plot_two_moons(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Network Wrapper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Bayesian network class has two internal states, dropout_state and stochastic_depth_state.\n",
    "    They overwrite the default behaviour of forward pass to enable switching between bayesian and deterministic modes.\n",
    "    Toggling the states is done by calling set_bayes_mode method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout_state: bool = False\n",
    "        self.stochastic_depth_state: bool = False\n",
    "\n",
    "    def _toggle_dropout(self) -> None:\n",
    "        # override the default behaviour of dropout layers\n",
    "        if self.dropout_state:\n",
    "            for m in self.modules():\n",
    "                if m.__class__.__name__.startswith(\"Dropout\"):\n",
    "                    m.train()\n",
    "\n",
    "    def _toggle_stochastic_depth(self) -> None:\n",
    "        # override the default behaviour of stochastic depth layers\n",
    "        if self.stochastic_depth_state:\n",
    "            for m in self.modules():\n",
    "                if m.__class__.__name__.startswith(\"StochasticDepth\"):\n",
    "                    m.train()\n",
    "\n",
    "    def _set_dropout(self, state: bool) -> None:\n",
    "        self.dropout_state = state\n",
    "\n",
    "    def _set_stochastic_depth(self, state: bool) -> None:\n",
    "        self.stochastic_depth_state = state\n",
    "\n",
    "    def set_bayes_mode(self, state: bool, mode: str) -> None:\n",
    "        if mode == \"dropout\":\n",
    "            self._set_dropout(state)\n",
    "        elif mode == \"sd\":\n",
    "            self._set_stochastic_depth(state)\n",
    "        else:\n",
    "            print(f\"Mode {mode} is not supported.\")\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        self._toggle_stochastic_depth()\n",
    "        self._toggle_dropout()\n",
    "\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple residual network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ResidualSdBlock(nn.Module):\n",
    "    \"\"\"Implements a simple x + lin(x) block\"\"\"\n",
    "\n",
    "    def __init__(self, features: int, p: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(features, features), nn.BatchNorm1d(features), nn.ReLU()\n",
    "        )\n",
    "        self.p = p\n",
    "        self.sd = StochasticDepth(p, mode=\"row\")\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = self.block(x)\n",
    "        out = self.sd(out)\n",
    "        # apply skip connection\n",
    "        out = x + out\n",
    "        return out\n",
    "    \n",
    "class SdNetwork(BayesianNet):\n",
    "    def __init__(self, inp: int, oup: int, N: int, d_feat: int, p=0.5):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            nn.Linear(inp, d_feat),\n",
    "            nn.BatchNorm1d(d_feat),\n",
    "            nn.ReLU(),\n",
    "            *[_ResidualSdBlock(d_feat, p=p) for _ in range(N)],\n",
    "            nn.Linear(d_feat, oup),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        return self.blocks(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple dropout network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ResidualDropoutBlock(nn.Module):\n",
    "    \"\"\"Implements a simple dropout(lin(x)) block\"\"\"\n",
    "\n",
    "    def __init__(self, features: int, p: float = 0.5) -> None:\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Linear(features, features),\n",
    "            nn.BatchNorm1d(features),\n",
    "            nn.Dropout1d(p=p),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        out = self.block(x)\n",
    "        # apply skip connection\n",
    "        out = x + out\n",
    "        return out\n",
    "\n",
    "class DONetwork(BayesianNet):\n",
    "    def __init__(self, inp: int, oup: int, N: int, d_feat: int, p=0.5):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            nn.Linear(inp, d_feat),\n",
    "            nn.BatchNorm1d(d_feat),\n",
    "            nn.ReLU(),\n",
    "            *[_ResidualDropoutBlock(d_feat, p=p) for _ in range(N)],\n",
    "            nn.Linear(d_feat, oup),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        return self.blocks(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_forward(x: Tensor, net: BayesianNet, mode: str, T: int) -> Tensor:\n",
    "    net.eval()\n",
    "    net.set_bayes_mode(True, mode)\n",
    "    with torch.no_grad():\n",
    "        y_logits_samples = net(x.repeat(T, 1))\n",
    "        y_logits = y_logits_samples.mean(dim=0)\n",
    "\n",
    "    return y_logits\n",
    "\n",
    "\n",
    "def batch_bayes_forward(x_batch: Tensor, net: BayesianNet, mode: str, T: int) -> Tensor:\n",
    "    \"\"\"\n",
    "    Performs bayesian forward pass in eval mode for a batch of input\n",
    "    Args\n",
    "    - net\n",
    "    - x_batch: tensor with shape (n, m) containing n samples of m dimensional inputs\n",
    "    - mode: 'dropout' or 'sd'\n",
    "    - T: number of stochastic samples\n",
    "\n",
    "    Returns\n",
    "    - y_logits_batch: tensor with shape (n, c)\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    net.set_bayes_mode(True, mode)\n",
    "    n = x_batch.shape[0]\n",
    "    with torch.no_grad():\n",
    "        y_logits_samples = net(x_batch.repeat(T, 1))  # (Txn, m)\n",
    "        y_logits_batch = y_logits_samples.reshape((T, n, -1))\n",
    "    net.set_bayes_mode(False, mode)\n",
    "\n",
    "    return y_logits_batch.mean(dim=0)\n",
    "\n",
    "\n",
    "def batch_ensemble_forward(x_batch: Tensor, nets: List[nn.Module]) -> Tensor:\n",
    "    for net in nets:\n",
    "        net.eval()\n",
    "    T = len(nets)\n",
    "    # use the first network to determine the shape of the output\n",
    "    y_logits = net(x_batch)  # n x c\n",
    "    y_logits_samples = torch.empty(T, *y_logits.shape)  # T x n x c\n",
    "    y_logits_samples[0, :] = y_logits\n",
    "    # iterate over the rest of the network\n",
    "    for i, net in enumerate(nets[1:]):\n",
    "        y_logits_samples[i + 1, :] = net(x_batch)\n",
    "\n",
    "    return y_logits_samples.mean(dim=0)  # n x c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train DO & sd networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net:BayesianNet = DONetwork(inp=2, oup=2, N=2, d_feat=30, p=0.5)\n",
    "net:BayesianNet = DONetwork(inp=2, oup=2, N=10, d_feat=10, p=0.5)\n",
    "# net:BayesianNet = SdNetwork(inp=2, oup=2, N=10, d_feat=10, p=0.5)\n",
    "net.train()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.05, momentum=0.9, weight_decay=1e-4)\n",
    "criterion = nn.BCELoss()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = []\n",
    "\n",
    "for epoch in tqdm(range(500)):\n",
    "    optimizer.zero_grad()\n",
    "    y_logits = net(X)\n",
    "    loss = criterion(y_logits.squeeze(1), Y.to(torch.long))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    history.append(loss.item())\n",
    "\n",
    "# plt.plot(history)\n",
    "    \n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    net.set_bayes_mode(False, \"sd\")\n",
    "    y_pred = net(X).argmax(dim=1)\n",
    "\n",
    "# plot_two_moons(X, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {(y_pred.squeeze() == Y).sum()/len(Y):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative view of confidence scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confidence(inp, forward_func, batch_size) -> Tensor:\n",
    "    \"\"\"Computes confidence score over the entire inp, handles batching\"\"\"\n",
    "    py = torch.empty((len(inp), 1), dtype=torch.float)\n",
    "    b = batch_size\n",
    "    for i in tqdm(range(0, inp.shape[0], b)):\n",
    "        x = inp[i : i + b]\n",
    "        logits = forward_func(x)\n",
    "        p, _ = torch.max(logits, dim=-1)\n",
    "        py[i : i + b] = p.unsqueeze(-1)\n",
    "    return py\n",
    "\n",
    "\n",
    "def plot_confidence(net: BayesianNet, forward_func, grid_size=100, batch_size=10):\n",
    "    net.eval()\n",
    "    XX, YY = np.meshgrid(np.linspace(-3, 3, grid_size), np.linspace(-3, 3, grid_size))\n",
    "    with torch.no_grad():\n",
    "        inp = torch.FloatTensor(np.stack((XX.ravel(), YY.ravel())).T)\n",
    "        # py, _ = torch.max(net(inp), dim=-1)\n",
    "        py = compute_confidence(inp, forward_func, batch_size)\n",
    "        conf = torch.where(py > 0.5, py, 1 - py)  ## THIS LINE COMPUTES THE CONFIDENCE\n",
    "        ZZ = conf.reshape(XX.shape)\n",
    "\n",
    "        plt.contourf(XX, YY, ZZ, cmap=\"PiYG\", vmin=0.5, vmax=1.2)\n",
    "        plt.colorbar(ticks=np.linspace(0.5, 1, 6))\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=\"autumn\", edgecolor=\"k\")\n",
    "        plt.title(\"Confidence plot\")\n",
    "\n",
    "\n",
    "forward_func = partial(batch_bayes_forward, net=net, mode=\"dropout\", T=30)\n",
    "plot_confidence(net, forward_func, grid_size=100, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
